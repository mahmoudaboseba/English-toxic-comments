{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52aedc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import SimpleRNN , Dropout ,LSTM,Conv1D, GlobalMaxPooling1D, MaxPooling1D , SpatialDropout1D,Activation,Dense, Dropout, Activation, Flatten, Input, concatenate , GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f265a686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ca88f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def replace_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def stem_words(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
    "\n",
    "def text2words(text):\n",
    "  return word_tokenize(text)\n",
    "\n",
    "def normalize_text( text):\n",
    "    text = remove_non_ascii(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = replace_numbers(text)\n",
    "    words = text2words(text)\n",
    "    words = remove_stopwords(words, stop_words)\n",
    "    words = lemmatize_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return ''.join(words)\n",
    "def normalize_corpus(corpus):\n",
    "  return [normalize_text(t) for t in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799e8c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('FinalBalancedDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e175239f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>@user camping tomorrow @user @user @user @use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>the next school year is the year for exams.ð...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>â #ireland consumer price index (mom) climb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>we are so selfish. #orlando #standwithorlando ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>i get to see my daddy today!!   #80days #getti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>ouch...junior is angryð#got7 #junior #yugyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>i am thankful for having a paner. #thankful #p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>its #friday! ð smiles all around via ig use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>as we all know, essential oils are not made of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  Toxic                                              Tweet\n",
       "0    0      0   @user when a father is dysfunctional and is s...\n",
       "1    1      0  @user @user thanks for #lyft credit i can't us...\n",
       "2    2      0                                bihday your majesty\n",
       "3    3      0  #model   i love u take with u all the time in ...\n",
       "4    4      0             factsguide: society now    #motivation\n",
       "5    5      0  [2/2] huge fan fare and big talking before the...\n",
       "6    6      0   @user camping tomorrow @user @user @user @use...\n",
       "7    7      0  the next school year is the year for exams.ð...\n",
       "8    8      0  we won!!! love the land!!! #allin #cavs #champ...\n",
       "9    9      0   @user @user welcome here !  i'm   it's so #gr...\n",
       "10  10      0   â #ireland consumer price index (mom) climb...\n",
       "11  11      0  we are so selfish. #orlando #standwithorlando ...\n",
       "12  12      0  i get to see my daddy today!!   #80days #getti...\n",
       "13  13      1  @user #cnn calls #michigan middle school 'buil...\n",
       "14  14      1  no comment!  in #australia   #opkillingbay #se...\n",
       "15  15      0  ouch...junior is angryð#got7 #junior #yugyo...\n",
       "16  16      0  i am thankful for having a paner. #thankful #p...\n",
       "17  17      1                             retweet if you agree! \n",
       "18  18      0  its #friday! ð smiles all around via ig use...\n",
       "19  19      0  as we all know, essential oils are not made of..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('FinalBalancedDataset.csv')\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74877eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.Tweet[:40000]\n",
    "y = dataset.Toxic[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9675f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be5b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_corpus(X_train)\n",
    "X_test = normalize_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb5257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1,3), tokenizer=text2words,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb32ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vec.fit_transform(X_train)\n",
    "X_test = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94e3173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf')\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6afb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf',gamma = 'auto',max_iter = 3000,C = 1)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score7 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "561ccf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'poly',gamma = 'auto',max_iter = 3000,C = 1)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score8 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "502d514d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'sigmoid',gamma = 'auto',max_iter = 3000,C = 1)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score9 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9157145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.19\n"
     ]
    }
   ],
   "source": [
    "print(score7*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1504b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.52\n"
     ]
    }
   ],
   "source": [
    "print(score8*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f306d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.71\n"
     ]
    }
   ],
   "source": [
    "print(score9*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83a0630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score1 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e350fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=3000)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score2 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c888faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=2500)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score3 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de189677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score4 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "187b47e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1500)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score5 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bf6173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=3500)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score6 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef262233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.51\n"
     ]
    }
   ],
   "source": [
    "print(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52ac207d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.78\n"
     ]
    }
   ],
   "source": [
    "print(score1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "742132af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.71000000000001\n"
     ]
    }
   ],
   "source": [
    "print(score2*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c02bc994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.72\n"
     ]
    }
   ],
   "source": [
    "print(score3*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85bf6b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.78\n"
     ]
    }
   ],
   "source": [
    "print(score4*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "350ae070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.78999999999999\n"
     ]
    }
   ],
   "source": [
    "print(score5*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23087eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93.78\n"
     ]
    }
   ],
   "source": [
    "print(score6*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "280e0a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck you!!\n",
      "abusive\n",
      "hello\n",
      "normal\n",
      "thank you so much\n",
      "normal\n",
      "ضعهف\n",
      "normal\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    inp = input()\n",
    "    if inp=='quit':\n",
    "        break\n",
    "    inp = [inp]\n",
    "    inp = vec.transform(inp)\n",
    "    #print(inp)\n",
    "    pred = classifier.predict(inp)\n",
    "    if(pred==1):\n",
    "        print('abusive')\n",
    "    else:\n",
    "        print('normal')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65904503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# to save the fitted model\n",
    "with open('SVM_model.pickle', 'wb') as model:\n",
    "    pickle.dump(classifier, model, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# to save the fitted tokenizer\n",
    "with open('vec_SVM.pickle', 'wb') as ve:\n",
    "    pickle.dump(vec, ve, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345c7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8aacfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a867ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = dataset.comment_text[:40000]\n",
    "y1 = dataset.toxic[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a540775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X1,y1,test_size = 0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f8658ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize_corpus(X_train)\n",
    "X_test = normalize_corpus(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92dc8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1,3), tokenizer=text2words,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6760f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vec.fit_transform(X_train)\n",
    "X_test = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f80bf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf',C=1000,probability = False)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb5f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.21\n"
     ]
    }
   ],
   "source": [
    "print(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd6a3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf',C=3000,probability = False)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fbede4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.21\n"
     ]
    }
   ],
   "source": [
    "print(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c52a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf')\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8463fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf',gamma = 'auto',max_iter = 3000,C = 1)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score7 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00d3a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'poly',gamma = 'auto',max_iter = 3000,C = 1)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score8 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fb27c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'sigmoid',gamma = 'auto',max_iter = 3000,C = 1)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score9 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50c49f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.84\n",
      "90.3\n",
      "90.3\n",
      "90.3\n"
     ]
    }
   ],
   "source": [
    "print(score*100)\n",
    "print(score7*100)\n",
    "print(score8*100)\n",
    "print(score9*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f65aa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score1 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41df0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=3000)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score2 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b27cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=2500)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score3 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e70b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score4 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c8e6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1500)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score5 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c27e7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=3500)\n",
    "clf.fit(X_train , y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score6 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05321caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.75\n",
      "94.72\n",
      "94.72\n",
      "94.73\n",
      "94.72\n",
      "94.73\n"
     ]
    }
   ],
   "source": [
    "print(score1*100)\n",
    "print(score2*100)\n",
    "print(score3*100)\n",
    "print(score4*100)\n",
    "print(score5*100)\n",
    "print(score6*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3809228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 277)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfc3cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 42)\n",
    "classifier.fit(X_train , y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7146e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5 , metric = 'minkowski' , p = 2)\n",
    "classifier.fit(X_train , y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "073a9761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "NB_model.fit(X_train,y_train)\n",
    "y_pred = NB_model.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(score*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
